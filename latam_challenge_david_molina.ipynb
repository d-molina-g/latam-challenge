{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89c67831",
   "metadata": {},
   "source": [
    "## Latam Challenge David Molina\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5094b1",
   "metadata": {},
   "source": [
    "Este cuaderno tiene como propósito abordar un challenge planteado mediante dos enfoques: uno que optimiza el uso de la memoria y otro que maximiza la eficiencia en el tiempo de ejecución.\n",
    "\n",
    "La base de datos a analizar consiste en un archivo de texto plano con registros en formato JSON separados por saltos de línea. Con un tamaño aproximado de 400 MB, este archivo no representa un desafío significativo para su análisis en una computadora personal, ya que la mayoría de los equipos actuales cuentan con suficiente memoria para manejarlo sin inconvenientes.\n",
    "\n",
    "Por lo antes mencionado, utilizaré mi computadora personal para la experimentación de este challenge, así como también evitar un uso innecesario de recursos. No obstante, en escenarios donde los archivos sean considerablemente más grandes, sería altamente recomendable emplear herramientas diseñadas para el procesamiento distribuido, como un clúster de Spark con DataProc en GCP o Amazon Glue en AWS. Estas soluciones permiten distribuir automáticamente los datos entre varios nodos en memoria o en disco, un principio que también se aplicará en este challenge, aunque utilizando archivos en una máquina local, como explicaré más adelante."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b65427",
   "metadata": {},
   "source": [
    "### Optimización"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f03072",
   "metadata": {},
   "source": [
    "Para el análisis de datos, utilizaremos la biblioteca pandas, que nos permitirá realizar agrupaciones y aplicar las transformaciones necesarias. Durante las pruebas de ejecución, se identificó que el principal cuello de botella en términos de tiempo y uso de memoria ocurría en la lectura y procesamiento del archivo JSON. Por esta razón, la optimización del código se ha centrado en mejorar tanto el rendimiento como el consumo de memoria en esta etapa crítica.\n",
    "\n",
    "A continuación, se detallan las librerías que se utilizarán en el desarrollo. Exceptuando pandas y emoji, todas las demás son librerías estándar que vienen incluidas en Python por defecto.\n",
    "\n",
    "Versión de Python: 3.10.6\n",
    "Paquetes:\n",
    "\n",
    "pandas==2.2.3\n",
    "emoji==2.14.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "849a1aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The memory_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext memory_profiler\n",
      "The line_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext line_profiler\n"
     ]
    }
   ],
   "source": [
    "# Importamos librerías\n",
    "from datetime import datetime\n",
    "from typing import List, Tuple\n",
    "import emoji\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "#Definimos el nombre de nuestro archivo JSON\n",
    "file_path = \"farmers-protest-tweets-2021-2-4.json\"\n",
    "\n",
    "# Definimos los comandos magic para medir tiempo y memoria\n",
    "%load_ext memory_profiler\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee5cbaf",
   "metadata": {},
   "source": [
    "### 1. Las top 10 fechas donde hay más tweets. Mencionar el usuario (username) que más publicaciones tiene\n",
    "\n",
    "Se determinó que el principal factor limitante es el tiempo requerido para leer y transformar los datos. No obstante, dado que no es necesario procesar la totalidad de los campos para resolver nuestro problema, se decidió focalizar el análisis únicamente en aquellos elementos imprescindibles. Para lograr un ahorro considerable en el tiempo de ejecución, se optó por cargar el archivo en su forma original y evitar el uso del método de lectura JSON de pandas, lo que permitió optimizar el rendimiento del proceso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9298b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q1_time(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
    "\n",
    "    # Abrimos el archivo y leemos todas las líneas\n",
    "    with open(file_path, 'r') as archivo:\n",
    "        lineas = archivo.readlines()\n",
    "    \n",
    "    registros = []\n",
    "    # Extraemos la información relevante de cada línea\n",
    "    for entrada in lineas:\n",
    "        dato = json.loads(entrada)\n",
    "        # Convertimos la fecha (se toman los primeros 10 caracteres) a objeto date\n",
    "        dia = datetime.strptime(dato[\"date\"][:10], \"%Y-%m-%d\").date()\n",
    "        nombre_usuario = dato[\"user\"][\"username\"]\n",
    "        id_tweet = dato[\"id\"]\n",
    "        registros.append({\"date\": dia, \"user\": nombre_usuario, \"id\": id_tweet})\n",
    "    \n",
    "    # Convertimos la lista de diccionarios en un DataFrame\n",
    "    df = pd.DataFrame(registros)\n",
    "    \n",
    "    # Agrupamos por fecha para contar los tweets y extraemos los 10 días con más actividad\n",
    "    resumen_dias = df.groupby(\"date\").count().sort_values(\"id\", ascending=False).head(10)\n",
    "    dias_destacados = list(resumen_dias.index)\n",
    "    \n",
    "    # Filtramos el DataFrame para incluir sólo los registros de los 10 días (top)\n",
    "    df_filtrado = df[df[\"date\"].isin(dias_destacados)]\n",
    "    \n",
    "    # Agrupamos por fecha y usuario para contar publicaciones, y seleccionar el usuario con mayor número de tweets en cada día\n",
    "    df_agrupado = df_filtrado.groupby([\"date\", \"user\"]).count().reset_index()\n",
    "    df_agrupado = df_agrupado.sort_values([\"date\", \"id\"], ascending=False)\n",
    "    df_resultado = df_agrupado.groupby(\"date\").first()\n",
    "    \n",
    "    # Convertimos el DataFrame resultante en una lista de tuplas (fecha, usuario)\n",
    "    salida = [ (registro.Index, registro.user) for registro in df_resultado[[\"user\"]].itertuples() ]\n",
    "    return salida\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6b698e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(datetime.date(2021, 2, 12), 'RanbirS00614606'), (datetime.date(2021, 2, 13), 'MaanDee08215437'), (datetime.date(2021, 2, 14), 'rebelpacifist'), (datetime.date(2021, 2, 15), 'jot__b'), (datetime.date(2021, 2, 16), 'jot__b'), (datetime.date(2021, 2, 17), 'RaaJVinderkaur'), (datetime.date(2021, 2, 18), 'neetuanjle_nitu'), (datetime.date(2021, 2, 19), 'Preetm91'), (datetime.date(2021, 2, 20), 'MangalJ23056160'), (datetime.date(2021, 2, 23), 'Surrypuria')]\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo de ejecución:\n",
    "print(q1_time(file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "638914e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q1_memory(file_path: str) -> list:\n",
    "\n",
    "    # Diccionario para contar cuántos registros (tweets) hay por fecha.\n",
    "    particiones = {}\n",
    "    \n",
    "    # Creamos un directorio temporal para almacenar los archivos particionados por fecha.\n",
    "    dir_tmp = \"tmp_q1\"\n",
    "    os.makedirs(dir_tmp, exist_ok=True)\n",
    "    \n",
    "    with open(file_path, 'r') as archivo_principal:\n",
    "        # Procesamos cada línea individualmente\n",
    "        for linea in archivo_principal:\n",
    "            registro = json.loads(linea)\n",
    "            fecha = registro.get(\"date\")[:10]\n",
    "            usuario = registro.get(\"user\").get(\"username\")\n",
    "            id_tweet = registro.get(\"id\")\n",
    "            datos = {\"usuario\": usuario, \"id\": id_tweet}\n",
    "            \n",
    "            # Generamos la ruta del archivo temporal para la fecha actual.\n",
    "            ruta_archivo_temp = os.path.join(dir_tmp, fecha)\n",
    "            with open(ruta_archivo_temp, \"a\") as archivo_temp:\n",
    "                archivo_temp.write(json.dumps(datos) + \"\\n\")\n",
    "            \n",
    "            # Actualizamos el contador de registros para la fecha correspondiente.\n",
    "            particiones[fecha] = particiones.get(fecha, 0) + 1\n",
    "    \n",
    "    # Convertimos el diccionario de particiones en un DataFrame para facilitar el ordenamiento.\n",
    "    df_particiones = pd.DataFrame([{\"fecha\": f, \"registros\": c} for f, c in particiones.items()])\n",
    "    # Seleccionamos los 10 días con mayor número de tweets.\n",
    "    df_top10 = df_particiones.sort_values(\"registros\", ascending=False).head(10)\n",
    "    fechas_top = list(df_top10[\"fecha\"])\n",
    "    \n",
    "    # Listamos para almacenar el resultado final: (fecha, usuario con más tweets ese día)\n",
    "    salida = []\n",
    "    for fecha in fechas_top:\n",
    "        ruta_temp = os.path.join(dir_tmp, fecha)\n",
    "        df_temp = pd.read_json(ruta_temp, lines=True)\n",
    "        \n",
    "        # Agrupamos por el campo 'usuario' y contar el número de tweets de cada uno.\n",
    "        # Ordenamos de forma descendente para identificar al usuario con mayor tweets.\n",
    "        df_usuario = df_temp.groupby(\"usuario\").count().sort_values(\"id\", ascending=False)\n",
    "        usuario_max = df_usuario.index[0]\n",
    "        \n",
    "        salida.append((fecha, usuario_max))\n",
    "    \n",
    "    # Limpiamos el directorio temporal eliminando todos los archivos generados.\n",
    "    for nombre_archivo in os.listdir(dir_tmp):\n",
    "        ruta_temp = os.path.join(dir_tmp, nombre_archivo)\n",
    "        if os.path.isfile(ruta_temp):\n",
    "            os.remove(ruta_temp)\n",
    "    \n",
    "    return salida\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35aa1577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('2021-02-12', 'RanbirS00614606'), ('2021-02-13', 'MaanDee08215437'), ('2021-02-17', 'RaaJVinderkaur'), ('2021-02-16', 'jot__b'), ('2021-02-14', 'rebelpacifist'), ('2021-02-18', 'neetuanjle_nitu'), ('2021-02-15', 'jot__b'), ('2021-02-20', 'MangalJ23056160'), ('2021-02-23', 'Surrypuria'), ('2021-02-19', 'Preetm91')]\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo de ejecución:\n",
    "print(q1_memory(file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3b78b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.69 s ± 17.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit q1_time(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2627189b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 579.79 MiB, increment: 38.62 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit q1_time(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0982ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.34 s ± 265 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "peak memory: 547.50 MiB, increment: 0.00 MiB\n"
     ]
    }
   ],
   "source": [
    "%timeit q1_memory(file_path)\n",
    "%memit q1_memory(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915357f8",
   "metadata": {},
   "source": [
    "| Función   | Tiempo   | Memoria   |\n",
    "|-----------|---------|-----------|\n",
    "| q1_time   | 2.69s   | 579.79 MiB |\n",
    "| q1_memory | 4.34s | 547.50 MiB  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a330ad7e",
   "metadata": {},
   "source": [
    "En este ejercicio, se implementó el algoritmo de dividir y vencer para optimizar el uso de memoria durante la lectura del archivo. Es decir, en lugar de cargar el archivo completo en memoria, se procedió a fragmentarlo en partes más pequeñas, permitiendo que cada segmento se procese de forma independiente. Esta estrategia facilitó el manejo de grandes volúmenes de datos al evitar cuellos de botella y posibilitar un procesamiento más eficiente, similar a las técnicas utilizadas en entornos de computación distribuida.\n",
    "\n",
    "Para este caso, y aplicable también a las funciones q2 y q3 de optimización de memoria, se optó por gestionar el procesamiento mediante archivos que actuaron como unidades independientes. En otras palabras, con la finalidad de generar un resumen diario del número de tweets por usuario, se leyó el archivo original de forma secuencial, dividiéndolo en distintos archivos separados por fecha, sobre los cuales se efectuaron los cálculos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8cdbb3",
   "metadata": {},
   "source": [
    "### 2. Los top 10 emojis más usados con su respectivo conteo:\n",
    "Para el análisis de emojis, aplicaremos una estrategia similar: limitaremos la lectura del archivo únicamente al campo relevante, en este caso, content. A partir de este campo, extraeremos los emojis utilizando la función analyze de la biblioteca emoji. Los resultados se almacenarán en una lista, que luego consolidaremos en una estructura global para todo el archivo. Finalmente, organizaremos los datos y realizaremos el conteo utilizando un DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c630d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q2_time(file_path: str) -> List[Tuple[str, int]]:\n",
    "\n",
    "    # Abrimos el archivo y leer todas las líneas\n",
    "    with open(file_path, 'r') as archivo:\n",
    "        lineas = archivo.readlines()\n",
    "    \n",
    "    # Recopilamos el contenido de cada registro\n",
    "    textos = []\n",
    "    for linea in lineas:\n",
    "        registro = json.loads(linea)\n",
    "        textos.append(registro.get(\"content\"))\n",
    "    \n",
    "    # Extraemos los emojis de cada texto y acumularlos en una lista\n",
    "    todos_emojis = []\n",
    "    for texto in textos:\n",
    "        # Asumimos que emoji.analyze devuelve objetos con el atributo 'chars'\n",
    "        todos_emojis.extend([item.chars for item in emoji.analyze(texto)])\n",
    "    \n",
    "    # Creamos un DataFrame con cada emoji y asignamos un contador inicial de 1\n",
    "    df_emojis = pd.DataFrame({\"emoji\": todos_emojis})\n",
    "    df_emojis[\"contador\"] = 1\n",
    "    \n",
    "    # Agrupamos por emoji, sumamos los contadores y ordenamos de mayor a menor\n",
    "    df_top = df_emojis.groupby(\"emoji\").sum().sort_values(\"contador\", ascending=False).head(10)\n",
    "    \n",
    "    # Reiniciamos índice para facilitar la conversión a lista de tuplas\n",
    "    df_resultado = df_top.reset_index()\n",
    "    # Convertimos a una lista de tuplas (emoji, cantidad)\n",
    "    salida = [(fila[\"emoji\"], fila[\"contador\"]) for _, fila in df_resultado.iterrows()]\n",
    "    \n",
    "    return salida\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "50710e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('🙏', 5049), ('😂', 3072), ('🚜', 2972), ('🌾', 2182), ('🇮🇳', 2086), ('🤣', 1668), ('✊', 1651), ('❤️', 1382), ('🙏🏻', 1317), ('💚', 1040)]\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo de ejecución:\n",
    "print(q2_time(file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c05f5324",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import emoji\n",
    "\n",
    "def q2_memory(file_path: str) -> list:\n",
    "\n",
    "    # Definimos el directorio temporal donde se guardará el archivo con los emojis extraídos.\n",
    "    dir_aux = \"tmp_q2\"\n",
    "    os.makedirs(dir_aux, exist_ok=True)  # Crea el directorio si no existe.\n",
    "\n",
    "    ruta_aux = os.path.join(dir_aux, \"emojis.txt\")\n",
    "    \n",
    "    # Abrir el archivo de entrada y el archivo temporal para escritura.\n",
    "    with open(file_path, 'r') as arch_in, open(ruta_aux, \"a\") as arch_out:\n",
    "        for linea in arch_in:\n",
    "            dato = json.loads(linea)\n",
    "            # Extraer el contenido del campo 'content', si existe.\n",
    "            texto = dato.get(\"content\", \"\")\n",
    "            # Obtenemos una lista de los emojis presentes en el texto.\n",
    "            lista_emojis = [item[\"emoji\"] for item in emoji.emoji_list(texto)]\n",
    "            # Si hay emojis en la línea, escribimos en el archivo temporal.\n",
    "            if lista_emojis:\n",
    "                arch_out.write(\"\\n\".join(lista_emojis) + \"\\n\")\n",
    "    \n",
    "    conteo_emojis = {}\n",
    "    \n",
    "    # Leemos el archivo temporal y contamos los emojis.\n",
    "    with open(ruta_aux, \"r\") as aux:\n",
    "        for linea in aux:\n",
    "            car = linea.strip()  # Eliminamos espacios en blanco y saltos de línea.\n",
    "            if car:  \n",
    "                conteo_emojis[car] = conteo_emojis.get(car, 0) + 1  \n",
    "\n",
    "    # Convertimos el diccionario de conteo a un DataFrame para facilitar el ordenamiento.\n",
    "    df_emo = pd.DataFrame({\"emoji\": list(conteo_emojis.keys()),\n",
    "                           \"cuenta\": list(conteo_emojis.values())})\n",
    "\n",
    "    # Ordenamos los emojis por frecuencia de uso en orden descendente y tomar los 10 más usados.\n",
    "    df_top = df_emo.sort_values(\"cuenta\", ascending=False).head(10)\n",
    "\n",
    "    # Convertimos el DataFrame en una lista de tuplas (emoji, conteo).\n",
    "    salida = [tuple(x) for x in df_top.set_index(\"emoji\").itertuples()]\n",
    "\n",
    "    # Eliminamos el archivo temporal después de procesar los datos.\n",
    "    os.remove(ruta_aux)\n",
    "\n",
    "    return salida\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e9d48915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('🙏', 5049), ('😂', 3072), ('🚜', 2972), ('🌾', 2182), ('🇮🇳', 2086), ('🤣', 1668), ('✊', 1651), ('❤️', 1382), ('🙏🏻', 1317), ('💚', 1040)]\n"
     ]
    }
   ],
   "source": [
    "print(q2_memory(file_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6359a7c8",
   "metadata": {},
   "source": [
    "Para llevar a cabo la ejecución con optimización de memoria, utilizamos archivos locales como paso previo en el procesamiento. Extraeremos los emojis de nuestra fuente de datos de manera secuencial, línea por línea, y procederemos a almacenarlos en un archivo local para luego agrupar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ecc55ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.94 s ± 193 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "peak memory: 571.87 MiB, increment: 6.12 MiB\n"
     ]
    }
   ],
   "source": [
    "%timeit q2_time(file_path)\n",
    "%memit q2_time(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "87ef3c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.37 s ± 225 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "peak memory: 567.81 MiB, increment: 0.00 MiB\n"
     ]
    }
   ],
   "source": [
    "%timeit q2_memory(file_path)\n",
    "%memit q2_memory(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50939a3",
   "metadata": {},
   "source": [
    "| Función   | Tiempo   | Memoria   |\n",
    "|-----------|---------|-----------|\n",
    "| q2_time   | 9.94s   | 571.87 MiB |\n",
    "| q2_memory | 9.06s | 567.81 MiB  |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa94cc2e",
   "metadata": {},
   "source": [
    "### 3. El top 10 histórico de usuarios (username) más influyentes en función del conteo de las menciones (@) que registra cada uno de ellos.\n",
    "En este problema, se tomará de igual manera los campos estrictamente necesarios, en este caso mentionedUsers el cual contiene un listado de usernames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "14f89797",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q3_time(file_path: str) -> List[Tuple[str, int]]:\n",
    "    # Abrimos el archivo y leer todas las líneas\n",
    "    with open(file_path, 'r') as f:\n",
    "        registros = f.readlines()\n",
    "    \n",
    "    # Lista para almacenar cada mención encontrada\n",
    "    lista_menciones = []\n",
    "    \n",
    "    # Recorremos cada registro y extraer las menciones\n",
    "    for linea in registros:\n",
    "        datos = json.loads(linea)\n",
    "        usuarios = datos.get(\"mentionedUsers\")\n",
    "        if usuarios:\n",
    "            # Agregamos cada nombre de usuario mencionado a la lista\n",
    "            for usuario in usuarios:\n",
    "                lista_menciones.append(usuario[\"username\"])\n",
    "    \n",
    "    # Creamos un DataFrame con las menciones para agrupar y sumar los conteos\n",
    "    df_menciones = pd.DataFrame({\"usuario\": lista_menciones})\n",
    "    df_menciones[\"conteo\"] = 1\n",
    "    # Agrupamos por 'usuario' y sumar los contadores\n",
    "    resumen = df_menciones.groupby(\"usuario\").sum()\n",
    "    # Ordenamos de mayor a menor cantidad y obtener los 10 primeros\n",
    "    top_10 = resumen.sort_values(\"conteo\", ascending=False).head(10)\n",
    "    \n",
    "    # Convertimos el DataFrame a una lista de tuplas (username, conteo)\n",
    "    salida = [(fila.Index, fila.conteo) for fila in top_10.itertuples()]\n",
    "    \n",
    "    return salida\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6e97d334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('narendramodi', 2265), ('Kisanektamorcha', 1840), ('RakeshTikaitBKU', 1644), ('PMOIndia', 1427), ('RahulGandhi', 1146), ('GretaThunberg', 1048), ('RaviSinghKA', 1019), ('rihanna', 986), ('UNHumanRights', 962), ('meenaharris', 926)]\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo de ejecución:\n",
    "print(q3_time(file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "12e666f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q3_memory(file_path: str) -> list:\n",
    "    \n",
    "    menciones = []  # Lista para almacenar los nombres de usuarios mencionados.\n",
    "    # Abrimos el archivo JSON y procesamos línea por línea.\n",
    "    with open(file_path, \"r\") as arch:\n",
    "        for linea in arch:\n",
    "            dato = json.loads(linea)\n",
    "            usuarios_mencionados = dato.get(\"mentionedUsers\")\n",
    "\n",
    "            if usuarios_mencionados:\n",
    "                # Extraemos los nombres de usuario de las menciones y agregarlos a la lista.\n",
    "                menciones.extend([u[\"username\"] for u in usuarios_mencionados])\n",
    "    \n",
    "    # DataFrame con la lista de menciones.\n",
    "    df_menc = pd.DataFrame({\"usuario\": menciones})\n",
    "    # Agregamos una columna auxiliar para el conteo de menciones.\n",
    "    df_menc[\"contador\"] = 1\n",
    "    # Agrupamos por usuario y para contar cuántas veces aparece cada uno.\n",
    "    df_agrup = df_menc.groupby(\"usuario\").sum()\n",
    "    # Ordenamos los usuarios por el número de menciones en orden descendente y tomar el top 10.\n",
    "    df_top = df_agrup.sort_values(\"contador\", ascending=False).head(10)\n",
    "    # Convertimos el DataFrame en una lista de tuplas (usuario, cantidad de menciones).\n",
    "    salida = [tuple(x) for x in df_top.itertuples()]\n",
    "\n",
    "    return salida\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d356fe8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('narendramodi', 2265), ('Kisanektamorcha', 1840), ('RakeshTikaitBKU', 1644), ('PMOIndia', 1427), ('RahulGandhi', 1146), ('GretaThunberg', 1048), ('RaviSinghKA', 1019), ('rihanna', 986), ('UNHumanRights', 962), ('meenaharris', 926)]\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo de ejecución:\n",
    "print(q3_memory(file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8a4554ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1 s ± 65.6 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "peak memory: 567.81 MiB, increment: 0.00 MiB\n"
     ]
    }
   ],
   "source": [
    "%timeit q3_time(file_path)\n",
    "%memit q3_time(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dd24b01f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.12 s ± 98.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "peak memory: 567.81 MiB, increment: 0.00 MiB\n"
     ]
    }
   ],
   "source": [
    "%timeit q3_memory(file_path)\n",
    "%memit q3_memory(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cff9085",
   "metadata": {},
   "source": [
    "| Función   | Tiempo   | Memoria   |\n",
    "|-----------|---------|-----------|\n",
    "| q3_time   | 2.1s   | 567.81 MiB |\n",
    "| q3_memory | 2.12s | 567.81 MiB  |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fb1ed8",
   "metadata": {},
   "source": [
    "### Conclusiones\n",
    "Hemos logrado optimizar el proceso de ejecución al identificar el cuello de botella en la lectura y el parseo de la información, lo que nos permitió abordar este paso crucial y ajustar la optimización según el objetivo deseado.\n",
    "\n",
    "Si bien la implementación se realizó en una máquina única, se aplicaron principios fundamentales de la computación distribuida, como el particionamiento y el procesamiento en lotes, que sientan las bases para una escalabilidad futura.\n",
    "\n",
    "Si bien en ciertos ejercicios la diferencia no es tan grande, podemos notar que esto radica especificamente en la maquina local y la memoria asignada, así como también el tamaño del archivo.  \n",
    "\n",
    "Para procesar archivos de mayor tamaño, es recomendable utilizar clusters de procesamiento, como Spark, que permiten distribuir los datos de manera óptima. Además, nuestros códigos pueden adaptarse fácilmente para trabajar con DataFrames en PySpark.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f542c32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python latam",
   "language": "python",
   "name": "latam"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
