{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89c67831",
   "metadata": {},
   "source": [
    "## Latam Challenge David Molina\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5094b1",
   "metadata": {},
   "source": [
    "Este cuaderno tiene como prop√≥sito abordar un challenge planteado mediante dos enfoques: uno que optimiza el uso de la memoria y otro que maximiza la eficiencia en el tiempo de ejecuci√≥n.\n",
    "\n",
    "La base de datos a analizar consiste en un archivo de texto plano con registros en formato JSON separados por saltos de l√≠nea. Con un tama√±o aproximado de 400 MB, este archivo no representa un desaf√≠o significativo para su an√°lisis en una computadora personal, ya que la mayor√≠a de los equipos actuales cuentan con suficiente memoria para manejarlo sin inconvenientes.\n",
    "\n",
    "Por lo antes mencionado, utilizar√© mi computadora personal para la experimentaci√≥n de este challenge, as√≠ como tambi√©n evitar un uso innecesario de recursos. No obstante, en escenarios donde los archivos sean considerablemente m√°s grandes, ser√≠a altamente recomendable emplear herramientas dise√±adas para el procesamiento distribuido, como un cl√∫ster de Spark con DataProc en GCP o Amazon Glue en AWS. Estas soluciones permiten distribuir autom√°ticamente los datos entre varios nodos en memoria o en disco, un principio que tambi√©n se aplicar√° en este challenge, aunque utilizando archivos en una m√°quina local, como explicar√© m√°s adelante."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b65427",
   "metadata": {},
   "source": [
    "### Optimizaci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f03072",
   "metadata": {},
   "source": [
    "Para el an√°lisis de datos, utilizaremos la biblioteca pandas, que nos permitir√° realizar agrupaciones y aplicar las transformaciones necesarias. Durante las pruebas de ejecuci√≥n, se identific√≥ que el principal cuello de botella en t√©rminos de tiempo y uso de memoria ocurr√≠a en la lectura y procesamiento del archivo JSON. Por esta raz√≥n, la optimizaci√≥n del c√≥digo se ha centrado en mejorar tanto el rendimiento como el consumo de memoria en esta etapa cr√≠tica.\n",
    "\n",
    "A continuaci√≥n, se detallan las librer√≠as que se utilizar√°n en el desarrollo. Exceptuando pandas y emoji, todas las dem√°s son librer√≠as est√°ndar que vienen incluidas en Python por defecto.\n",
    "\n",
    "Versi√≥n de Python: 3.10.6\n",
    "Paquetes:\n",
    "\n",
    "pandas==2.2.3\n",
    "emoji==2.14.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "849a1aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The memory_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext memory_profiler\n",
      "The line_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext line_profiler\n"
     ]
    }
   ],
   "source": [
    "# Importamos librer√≠as\n",
    "from datetime import datetime\n",
    "from typing import List, Tuple\n",
    "import emoji\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "#Definimos el nombre de nuestro archivo JSON\n",
    "file_path = \"farmers-protest-tweets-2021-2-4.json\"\n",
    "\n",
    "# Definimos los comandos magic para medir tiempo y memoria\n",
    "%load_ext memory_profiler\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee5cbaf",
   "metadata": {},
   "source": [
    "### 1. Las top 10 fechas donde hay m√°s tweets. Mencionar el usuario (username) que m√°s publicaciones tiene\n",
    "\n",
    "Se determin√≥ que el principal factor limitante es el tiempo requerido para leer y transformar los datos. No obstante, dado que no es necesario procesar la totalidad de los campos para resolver nuestro problema, se decidi√≥ focalizar el an√°lisis √∫nicamente en aquellos elementos imprescindibles. Para lograr un ahorro considerable en el tiempo de ejecuci√≥n, se opt√≥ por cargar el archivo en su forma original y evitar el uso del m√©todo de lectura JSON de pandas, lo que permiti√≥ optimizar el rendimiento del proceso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9298b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q1_time(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
    "\n",
    "    # Abrimos el archivo y leemos todas las l√≠neas\n",
    "    with open(file_path, 'r') as archivo:\n",
    "        lineas = archivo.readlines()\n",
    "    \n",
    "    registros = []\n",
    "    # Extraemos la informaci√≥n relevante de cada l√≠nea\n",
    "    for entrada in lineas:\n",
    "        dato = json.loads(entrada)\n",
    "        # Convertimos la fecha (se toman los primeros 10 caracteres) a objeto date\n",
    "        dia = datetime.strptime(dato[\"date\"][:10], \"%Y-%m-%d\").date()\n",
    "        nombre_usuario = dato[\"user\"][\"username\"]\n",
    "        id_tweet = dato[\"id\"]\n",
    "        registros.append({\"date\": dia, \"user\": nombre_usuario, \"id\": id_tweet})\n",
    "    \n",
    "    # Convertimos la lista de diccionarios en un DataFrame\n",
    "    df = pd.DataFrame(registros)\n",
    "    \n",
    "    # Agrupamos por fecha para contar los tweets y extraemos los 10 d√≠as con m√°s actividad\n",
    "    resumen_dias = df.groupby(\"date\").count().sort_values(\"id\", ascending=False).head(10)\n",
    "    dias_destacados = list(resumen_dias.index)\n",
    "    \n",
    "    # Filtramos el DataFrame para incluir s√≥lo los registros de los 10 d√≠as (top)\n",
    "    df_filtrado = df[df[\"date\"].isin(dias_destacados)]\n",
    "    \n",
    "    # Agrupamos por fecha y usuario para contar publicaciones, y seleccionar el usuario con mayor n√∫mero de tweets en cada d√≠a\n",
    "    df_agrupado = df_filtrado.groupby([\"date\", \"user\"]).count().reset_index()\n",
    "    df_agrupado = df_agrupado.sort_values([\"date\", \"id\"], ascending=False)\n",
    "    df_resultado = df_agrupado.groupby(\"date\").first()\n",
    "    \n",
    "    # Convertimos el DataFrame resultante en una lista de tuplas (fecha, usuario)\n",
    "    salida = [ (registro.Index, registro.user) for registro in df_resultado[[\"user\"]].itertuples() ]\n",
    "    return salida\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6b698e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(datetime.date(2021, 2, 12), 'RanbirS00614606'), (datetime.date(2021, 2, 13), 'MaanDee08215437'), (datetime.date(2021, 2, 14), 'rebelpacifist'), (datetime.date(2021, 2, 15), 'jot__b'), (datetime.date(2021, 2, 16), 'jot__b'), (datetime.date(2021, 2, 17), 'RaaJVinderkaur'), (datetime.date(2021, 2, 18), 'neetuanjle_nitu'), (datetime.date(2021, 2, 19), 'Preetm91'), (datetime.date(2021, 2, 20), 'MangalJ23056160'), (datetime.date(2021, 2, 23), 'Surrypuria')]\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo de ejecuci√≥n:\n",
    "print(q1_time(file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "638914e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q1_memory(file_path: str) -> list:\n",
    "\n",
    "    # Diccionario para contar cu√°ntos registros (tweets) hay por fecha.\n",
    "    particiones = {}\n",
    "    \n",
    "    # Creamos un directorio temporal para almacenar los archivos particionados por fecha.\n",
    "    dir_tmp = \"tmp_q1\"\n",
    "    os.makedirs(dir_tmp, exist_ok=True)\n",
    "    \n",
    "    with open(file_path, 'r') as archivo_principal:\n",
    "        # Procesamos cada l√≠nea individualmente\n",
    "        for linea in archivo_principal:\n",
    "            registro = json.loads(linea)\n",
    "            fecha = registro.get(\"date\")[:10]\n",
    "            usuario = registro.get(\"user\").get(\"username\")\n",
    "            id_tweet = registro.get(\"id\")\n",
    "            datos = {\"usuario\": usuario, \"id\": id_tweet}\n",
    "            \n",
    "            # Generamos la ruta del archivo temporal para la fecha actual.\n",
    "            ruta_archivo_temp = os.path.join(dir_tmp, fecha)\n",
    "            with open(ruta_archivo_temp, \"a\") as archivo_temp:\n",
    "                archivo_temp.write(json.dumps(datos) + \"\\n\")\n",
    "            \n",
    "            # Actualizamos el contador de registros para la fecha correspondiente.\n",
    "            particiones[fecha] = particiones.get(fecha, 0) + 1\n",
    "    \n",
    "    # Convertimos el diccionario de particiones en un DataFrame para facilitar el ordenamiento.\n",
    "    df_particiones = pd.DataFrame([{\"fecha\": f, \"registros\": c} for f, c in particiones.items()])\n",
    "    # Seleccionamos los 10 d√≠as con mayor n√∫mero de tweets.\n",
    "    df_top10 = df_particiones.sort_values(\"registros\", ascending=False).head(10)\n",
    "    fechas_top = list(df_top10[\"fecha\"])\n",
    "    \n",
    "    # Listamos para almacenar el resultado final: (fecha, usuario con m√°s tweets ese d√≠a)\n",
    "    salida = []\n",
    "    for fecha in fechas_top:\n",
    "        ruta_temp = os.path.join(dir_tmp, fecha)\n",
    "        df_temp = pd.read_json(ruta_temp, lines=True)\n",
    "        \n",
    "        # Agrupamos por el campo 'usuario' y contar el n√∫mero de tweets de cada uno.\n",
    "        # Ordenamos de forma descendente para identificar al usuario con mayor tweets.\n",
    "        df_usuario = df_temp.groupby(\"usuario\").count().sort_values(\"id\", ascending=False)\n",
    "        usuario_max = df_usuario.index[0]\n",
    "        \n",
    "        salida.append((fecha, usuario_max))\n",
    "    \n",
    "    # Limpiamos el directorio temporal eliminando todos los archivos generados.\n",
    "    for nombre_archivo in os.listdir(dir_tmp):\n",
    "        ruta_temp = os.path.join(dir_tmp, nombre_archivo)\n",
    "        if os.path.isfile(ruta_temp):\n",
    "            os.remove(ruta_temp)\n",
    "    \n",
    "    return salida\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35aa1577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('2021-02-12', 'RanbirS00614606'), ('2021-02-13', 'MaanDee08215437'), ('2021-02-17', 'RaaJVinderkaur'), ('2021-02-16', 'jot__b'), ('2021-02-14', 'rebelpacifist'), ('2021-02-18', 'neetuanjle_nitu'), ('2021-02-15', 'jot__b'), ('2021-02-20', 'MangalJ23056160'), ('2021-02-23', 'Surrypuria'), ('2021-02-19', 'Preetm91')]\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo de ejecuci√≥n:\n",
    "print(q1_memory(file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3b78b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.69 s ¬± 17.9 ms per loop (mean ¬± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit q1_time(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2627189b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 579.79 MiB, increment: 38.62 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit q1_time(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0982ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.34 s ¬± 265 ms per loop (mean ¬± std. dev. of 7 runs, 1 loop each)\n",
      "peak memory: 547.50 MiB, increment: 0.00 MiB\n"
     ]
    }
   ],
   "source": [
    "%timeit q1_memory(file_path)\n",
    "%memit q1_memory(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915357f8",
   "metadata": {},
   "source": [
    "| Funci√≥n   | Tiempo   | Memoria   |\n",
    "|-----------|---------|-----------|\n",
    "| q1_time   | 2.69s   | 579.79 MiB |\n",
    "| q1_memory | 4.34s | 547.50 MiB  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a330ad7e",
   "metadata": {},
   "source": [
    "En este ejercicio, se implement√≥ el algoritmo de dividir y vencer para optimizar el uso de memoria durante la lectura del archivo. Es decir, en lugar de cargar el archivo completo en memoria, se procedi√≥ a fragmentarlo en partes m√°s peque√±as, permitiendo que cada segmento se procese de forma independiente. Esta estrategia facilit√≥ el manejo de grandes vol√∫menes de datos al evitar cuellos de botella y posibilitar un procesamiento m√°s eficiente, similar a las t√©cnicas utilizadas en entornos de computaci√≥n distribuida.\n",
    "\n",
    "Para este caso, y aplicable tambi√©n a las funciones q2 y q3 de optimizaci√≥n de memoria, se opt√≥ por gestionar el procesamiento mediante archivos que actuaron como unidades independientes. En otras palabras, con la finalidad de generar un resumen diario del n√∫mero de tweets por usuario, se ley√≥ el archivo original de forma secuencial, dividi√©ndolo en distintos archivos separados por fecha, sobre los cuales se efectuaron los c√°lculos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8cdbb3",
   "metadata": {},
   "source": [
    "### 2. Los top 10 emojis m√°s usados con su respectivo conteo:\n",
    "Para el an√°lisis de emojis, aplicaremos una estrategia similar: limitaremos la lectura del archivo √∫nicamente al campo relevante, en este caso, content. A partir de este campo, extraeremos los emojis utilizando la funci√≥n analyze de la biblioteca emoji. Los resultados se almacenar√°n en una lista, que luego consolidaremos en una estructura global para todo el archivo. Finalmente, organizaremos los datos y realizaremos el conteo utilizando un DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c630d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q2_time(file_path: str) -> List[Tuple[str, int]]:\n",
    "\n",
    "    # Abrimos el archivo y leer todas las l√≠neas\n",
    "    with open(file_path, 'r') as archivo:\n",
    "        lineas = archivo.readlines()\n",
    "    \n",
    "    # Recopilamos el contenido de cada registro\n",
    "    textos = []\n",
    "    for linea in lineas:\n",
    "        registro = json.loads(linea)\n",
    "        textos.append(registro.get(\"content\"))\n",
    "    \n",
    "    # Extraemos los emojis de cada texto y acumularlos en una lista\n",
    "    todos_emojis = []\n",
    "    for texto in textos:\n",
    "        # Asumimos que emoji.analyze devuelve objetos con el atributo 'chars'\n",
    "        todos_emojis.extend([item.chars for item in emoji.analyze(texto)])\n",
    "    \n",
    "    # Creamos un DataFrame con cada emoji y asignamos un contador inicial de 1\n",
    "    df_emojis = pd.DataFrame({\"emoji\": todos_emojis})\n",
    "    df_emojis[\"contador\"] = 1\n",
    "    \n",
    "    # Agrupamos por emoji, sumamos los contadores y ordenamos de mayor a menor\n",
    "    df_top = df_emojis.groupby(\"emoji\").sum().sort_values(\"contador\", ascending=False).head(10)\n",
    "    \n",
    "    # Reiniciamos √≠ndice para facilitar la conversi√≥n a lista de tuplas\n",
    "    df_resultado = df_top.reset_index()\n",
    "    # Convertimos a una lista de tuplas (emoji, cantidad)\n",
    "    salida = [(fila[\"emoji\"], fila[\"contador\"]) for _, fila in df_resultado.iterrows()]\n",
    "    \n",
    "    return salida\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "50710e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('üôè', 5049), ('üòÇ', 3072), ('üöú', 2972), ('üåæ', 2182), ('üáÆüá≥', 2086), ('ü§£', 1668), ('‚úä', 1651), ('‚ù§Ô∏è', 1382), ('üôèüèª', 1317), ('üíö', 1040)]\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo de ejecuci√≥n:\n",
    "print(q2_time(file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c05f5324",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import emoji\n",
    "\n",
    "def q2_memory(file_path: str) -> list:\n",
    "\n",
    "    # Definimos el directorio temporal donde se guardar√° el archivo con los emojis extra√≠dos.\n",
    "    dir_aux = \"tmp_q2\"\n",
    "    os.makedirs(dir_aux, exist_ok=True)  # Crea el directorio si no existe.\n",
    "\n",
    "    ruta_aux = os.path.join(dir_aux, \"emojis.txt\")\n",
    "    \n",
    "    # Abrir el archivo de entrada y el archivo temporal para escritura.\n",
    "    with open(file_path, 'r') as arch_in, open(ruta_aux, \"a\") as arch_out:\n",
    "        for linea in arch_in:\n",
    "            dato = json.loads(linea)\n",
    "            # Extraer el contenido del campo 'content', si existe.\n",
    "            texto = dato.get(\"content\", \"\")\n",
    "            # Obtenemos una lista de los emojis presentes en el texto.\n",
    "            lista_emojis = [item[\"emoji\"] for item in emoji.emoji_list(texto)]\n",
    "            # Si hay emojis en la l√≠nea, escribimos en el archivo temporal.\n",
    "            if lista_emojis:\n",
    "                arch_out.write(\"\\n\".join(lista_emojis) + \"\\n\")\n",
    "    \n",
    "    conteo_emojis = {}\n",
    "    \n",
    "    # Leemos el archivo temporal y contamos los emojis.\n",
    "    with open(ruta_aux, \"r\") as aux:\n",
    "        for linea in aux:\n",
    "            car = linea.strip()  # Eliminamos espacios en blanco y saltos de l√≠nea.\n",
    "            if car:  \n",
    "                conteo_emojis[car] = conteo_emojis.get(car, 0) + 1  \n",
    "\n",
    "    # Convertimos el diccionario de conteo a un DataFrame para facilitar el ordenamiento.\n",
    "    df_emo = pd.DataFrame({\"emoji\": list(conteo_emojis.keys()),\n",
    "                           \"cuenta\": list(conteo_emojis.values())})\n",
    "\n",
    "    # Ordenamos los emojis por frecuencia de uso en orden descendente y tomar los 10 m√°s usados.\n",
    "    df_top = df_emo.sort_values(\"cuenta\", ascending=False).head(10)\n",
    "\n",
    "    # Convertimos el DataFrame en una lista de tuplas (emoji, conteo).\n",
    "    salida = [tuple(x) for x in df_top.set_index(\"emoji\").itertuples()]\n",
    "\n",
    "    # Eliminamos el archivo temporal despu√©s de procesar los datos.\n",
    "    os.remove(ruta_aux)\n",
    "\n",
    "    return salida\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e9d48915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('üôè', 5049), ('üòÇ', 3072), ('üöú', 2972), ('üåæ', 2182), ('üáÆüá≥', 2086), ('ü§£', 1668), ('‚úä', 1651), ('‚ù§Ô∏è', 1382), ('üôèüèª', 1317), ('üíö', 1040)]\n"
     ]
    }
   ],
   "source": [
    "print(q2_memory(file_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6359a7c8",
   "metadata": {},
   "source": [
    "Para llevar a cabo la ejecuci√≥n con optimizaci√≥n de memoria, utilizamos archivos locales como paso previo en el procesamiento. Extraeremos los emojis de nuestra fuente de datos de manera secuencial, l√≠nea por l√≠nea, y procederemos a almacenarlos en un archivo local para luego agrupar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ecc55ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.94 s ¬± 193 ms per loop (mean ¬± std. dev. of 7 runs, 1 loop each)\n",
      "peak memory: 571.87 MiB, increment: 6.12 MiB\n"
     ]
    }
   ],
   "source": [
    "%timeit q2_time(file_path)\n",
    "%memit q2_time(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "87ef3c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.37 s ¬± 225 ms per loop (mean ¬± std. dev. of 7 runs, 1 loop each)\n",
      "peak memory: 567.81 MiB, increment: 0.00 MiB\n"
     ]
    }
   ],
   "source": [
    "%timeit q2_memory(file_path)\n",
    "%memit q2_memory(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50939a3",
   "metadata": {},
   "source": [
    "| Funci√≥n   | Tiempo   | Memoria   |\n",
    "|-----------|---------|-----------|\n",
    "| q2_time   | 9.94s   | 571.87 MiB |\n",
    "| q2_memory | 9.06s | 567.81 MiB  |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa94cc2e",
   "metadata": {},
   "source": [
    "### 3. El top 10 hist√≥rico de usuarios (username) m√°s influyentes en funci√≥n del conteo de las menciones (@) que registra cada uno de ellos.\n",
    "En este problema, se tomar√° de igual manera los campos estrictamente necesarios, en este caso mentionedUsers el cual contiene un listado de usernames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "14f89797",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q3_time(file_path: str) -> List[Tuple[str, int]]:\n",
    "    # Abrimos el archivo y leer todas las l√≠neas\n",
    "    with open(file_path, 'r') as f:\n",
    "        registros = f.readlines()\n",
    "    \n",
    "    # Lista para almacenar cada menci√≥n encontrada\n",
    "    lista_menciones = []\n",
    "    \n",
    "    # Recorremos cada registro y extraer las menciones\n",
    "    for linea in registros:\n",
    "        datos = json.loads(linea)\n",
    "        usuarios = datos.get(\"mentionedUsers\")\n",
    "        if usuarios:\n",
    "            # Agregamos cada nombre de usuario mencionado a la lista\n",
    "            for usuario in usuarios:\n",
    "                lista_menciones.append(usuario[\"username\"])\n",
    "    \n",
    "    # Creamos un DataFrame con las menciones para agrupar y sumar los conteos\n",
    "    df_menciones = pd.DataFrame({\"usuario\": lista_menciones})\n",
    "    df_menciones[\"conteo\"] = 1\n",
    "    # Agrupamos por 'usuario' y sumar los contadores\n",
    "    resumen = df_menciones.groupby(\"usuario\").sum()\n",
    "    # Ordenamos de mayor a menor cantidad y obtener los 10 primeros\n",
    "    top_10 = resumen.sort_values(\"conteo\", ascending=False).head(10)\n",
    "    \n",
    "    # Convertimos el DataFrame a una lista de tuplas (username, conteo)\n",
    "    salida = [(fila.Index, fila.conteo) for fila in top_10.itertuples()]\n",
    "    \n",
    "    return salida\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6e97d334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('narendramodi', 2265), ('Kisanektamorcha', 1840), ('RakeshTikaitBKU', 1644), ('PMOIndia', 1427), ('RahulGandhi', 1146), ('GretaThunberg', 1048), ('RaviSinghKA', 1019), ('rihanna', 986), ('UNHumanRights', 962), ('meenaharris', 926)]\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo de ejecuci√≥n:\n",
    "print(q3_time(file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "12e666f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q3_memory(file_path: str) -> list:\n",
    "    \n",
    "    menciones = []  # Lista para almacenar los nombres de usuarios mencionados.\n",
    "    # Abrimos el archivo JSON y procesamos l√≠nea por l√≠nea.\n",
    "    with open(file_path, \"r\") as arch:\n",
    "        for linea in arch:\n",
    "            dato = json.loads(linea)\n",
    "            usuarios_mencionados = dato.get(\"mentionedUsers\")\n",
    "\n",
    "            if usuarios_mencionados:\n",
    "                # Extraemos los nombres de usuario de las menciones y agregarlos a la lista.\n",
    "                menciones.extend([u[\"username\"] for u in usuarios_mencionados])\n",
    "    \n",
    "    # DataFrame con la lista de menciones.\n",
    "    df_menc = pd.DataFrame({\"usuario\": menciones})\n",
    "    # Agregamos una columna auxiliar para el conteo de menciones.\n",
    "    df_menc[\"contador\"] = 1\n",
    "    # Agrupamos por usuario y para contar cu√°ntas veces aparece cada uno.\n",
    "    df_agrup = df_menc.groupby(\"usuario\").sum()\n",
    "    # Ordenamos los usuarios por el n√∫mero de menciones en orden descendente y tomar el top 10.\n",
    "    df_top = df_agrup.sort_values(\"contador\", ascending=False).head(10)\n",
    "    # Convertimos el DataFrame en una lista de tuplas (usuario, cantidad de menciones).\n",
    "    salida = [tuple(x) for x in df_top.itertuples()]\n",
    "\n",
    "    return salida\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d356fe8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('narendramodi', 2265), ('Kisanektamorcha', 1840), ('RakeshTikaitBKU', 1644), ('PMOIndia', 1427), ('RahulGandhi', 1146), ('GretaThunberg', 1048), ('RaviSinghKA', 1019), ('rihanna', 986), ('UNHumanRights', 962), ('meenaharris', 926)]\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo de ejecuci√≥n:\n",
    "print(q3_memory(file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8a4554ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1 s ¬± 65.6 ms per loop (mean ¬± std. dev. of 7 runs, 1 loop each)\n",
      "peak memory: 567.81 MiB, increment: 0.00 MiB\n"
     ]
    }
   ],
   "source": [
    "%timeit q3_time(file_path)\n",
    "%memit q3_time(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dd24b01f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.12 s ¬± 98.5 ms per loop (mean ¬± std. dev. of 7 runs, 1 loop each)\n",
      "peak memory: 567.81 MiB, increment: 0.00 MiB\n"
     ]
    }
   ],
   "source": [
    "%timeit q3_memory(file_path)\n",
    "%memit q3_memory(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cff9085",
   "metadata": {},
   "source": [
    "| Funci√≥n   | Tiempo   | Memoria   |\n",
    "|-----------|---------|-----------|\n",
    "| q3_time   | 2.1s   | 567.81 MiB |\n",
    "| q3_memory | 2.12s | 567.81 MiB  |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fb1ed8",
   "metadata": {},
   "source": [
    "### Conclusiones\n",
    "Hemos logrado optimizar el proceso de ejecuci√≥n al identificar el cuello de botella en la lectura y el parseo de la informaci√≥n, lo que nos permiti√≥ abordar este paso crucial y ajustar la optimizaci√≥n seg√∫n el objetivo deseado.\n",
    "\n",
    "Si bien la implementaci√≥n se realiz√≥ en una m√°quina √∫nica, se aplicaron principios fundamentales de la computaci√≥n distribuida, como el particionamiento y el procesamiento en lotes, que sientan las bases para una escalabilidad futura.\n",
    "\n",
    "Si bien en ciertos ejercicios la diferencia no es tan grande, podemos notar que esto radica especificamente en la maquina local y la memoria asignada, as√≠ como tambi√©n el tama√±o del archivo.  \n",
    "\n",
    "Para procesar archivos de mayor tama√±o, es recomendable utilizar clusters de procesamiento, como Spark, que permiten distribuir los datos de manera √≥ptima. Adem√°s, nuestros c√≥digos pueden adaptarse f√°cilmente para trabajar con DataFrames en PySpark.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f542c32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python latam",
   "language": "python",
   "name": "latam"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
